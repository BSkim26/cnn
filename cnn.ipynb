{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "train_loss = []   \n",
        "best_rmse = []   \n",
        "val_img_loss = []\n",
        "val_img_rmse = []"
      ],
      "metadata": {
        "id": "Ua_0gYr8Lzq0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2bcba5f-002e-4f49-9622-ada6117faf0a"
      },
      "source": [
        "## Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2b0d9b68-7102-4eca-9543-3b9b8acafc6e"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import random                                       # random 모듈 가져오기\n",
        "import pandas as pd                                 # pandas 모듈 가져오기\n",
        "import numpy as np                                  # numpy 모듈 가져오기\n",
        "import os                                           # os 모듈 가져오기\n",
        "import glob                                         # glob 모듈 가져오기\n",
        "import cv2                                          # cv2 모듈 가져오기\n",
        "\n",
        "import torch                                        # torch 모듈 가져오기\n",
        "import torch.nn as nn                               # torch.nn 모듈(데이터 구조나 레이어 등이 정의) 가져오기\n",
        "import torch.optim as optim                         # torch.optim(최적화 알고리즘이 구현) 가져오기\n",
        "import torch.nn.functional as F                     # nn을 함수형으로 가져오기\n",
        "from torch.utils.data import Dataset, DataLoader    # 데이터를 python으로 연결할 수 있도록 설정\n",
        "\n",
        "from tqdm.auto import tqdm                          # 깔끔하게 출력하기 위해 tqdm.auto에서 tqdm 가져오기\n",
        "from sklearn.metrics import mean_squared_error      # 성능 평가 지표 함수 가져오기\n",
        "\n",
        "import warnings                                     # 경고 제어를 위해 가져오기\n",
        "warnings.filterwarnings(action='ignore')            # 경고 메세지 무시\n",
        " \n",
        "from google.colab.patches import cv2_imshow         # google.colab.patches에서 cv2_imshow를 사용하기 위해 불러옴\n",
        "\n",
        "from torchvision import transforms                  # 이미지 변환을 위해 transform을 불러옴\n",
        "\n",
        "import PIL                                          # 이미지\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d13862e3-bb27-47af-9b58-a9fbf804df71"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')   # cuda를 사용하기 위해 cuda로 설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUfhiWZDhFT5",
        "outputId": "7d7a50ff-a72f-4d70-a876-faa2ca064750"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive              # 구글 drive 불러오기\n",
        "drive.mount('/content/drive')               # 구글 코랩환경으로 실험 진행"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3LXxBurUQ3Ap",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2db0f09-ba6a-4105-b75b-a480617cff5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Dec 17 08:28:23 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   46C    P8    10W /  70W |      3MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi                                 # 사용하는 프로세서 확인"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KHkrlyBKhwSK"
      },
      "outputs": [],
      "source": [
        "!unzip -qq \"/content/drive/MyDrive/open.zip\"    # 구글 드라이브에 있는 데이터 알집을 풀고 실험 진행"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3367399-9798-4e38-967b-fd2320b9a2b2"
      },
      "outputs": [],
      "source": [
        "CFG = {                                         # 동일한 parameter로 진행하기 위해 dictonary로 값 저장\n",
        "    'WIDTH':48,\n",
        "    'HEIGHT':72,\n",
        "    'EPOCHS':2,\n",
        "    'LEARNING_RATE':1e-3,\n",
        "    'BATCH_SIZE':128,\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "867AM7JbInfg"
      },
      "source": [
        "## 데이터 전처리\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQmHcLR9Iraq"
      },
      "outputs": [],
      "source": [
        "# case 1 \n",
        "sem_1_80_paths = sorted(glob.glob('./simulation_data/SEM/Case_1/80/*.png'))\n",
        "sem_1_81_paths = sorted(glob.glob('./simulation_data/SEM/Case_1/81/*.png'))\n",
        "sem_1_82_paths = sorted(glob.glob('./simulation_data/SEM/Case_1/82/*.png'))\n",
        "sem_1_83_paths = sorted(glob.glob('./simulation_data/SEM/Case_1/83/*.png'))\n",
        "sem_1_84_paths = sorted(glob.glob('./simulation_data/SEM/Case_1/84/*.png'))\n",
        "\n",
        "# case 2\n",
        "sem_2_80_paths = sorted(glob.glob('./simulation_data/SEM/Case_2/80/*.png'))\n",
        "sem_2_81_paths = sorted(glob.glob('./simulation_data/SEM/Case_2/81/*.png'))\n",
        "sem_2_82_paths = sorted(glob.glob('./simulation_data/SEM/Case_2/82/*.png'))\n",
        "sem_2_83_paths = sorted(glob.glob('./simulation_data/SEM/Case_2/83/*.png'))\n",
        "sem_2_84_paths = sorted(glob.glob('./simulation_data/SEM/Case_2/84/*.png'))\n",
        "\n",
        "# case 3\n",
        "sem_3_80_paths = sorted(glob.glob('./simulation_data/SEM/Case_3/80/*.png'))\n",
        "sem_3_81_paths = sorted(glob.glob('./simulation_data/SEM/Case_3/81/*.png'))\n",
        "sem_3_82_paths = sorted(glob.glob('./simulation_data/SEM/Case_3/82/*.png'))\n",
        "sem_3_83_paths = sorted(glob.glob('./simulation_data/SEM/Case_3/83/*.png'))\n",
        "sem_3_84_paths = sorted(glob.glob('./simulation_data/SEM/Case_3/84/*.png'))\n",
        "\n",
        "# case 4\n",
        "sem_4_80_paths = sorted(glob.glob('./simulation_data/SEM/Case_4/80/*.png'))\n",
        "sem_4_81_paths = sorted(glob.glob('./simulation_data/SEM/Case_4/81/*.png'))\n",
        "sem_4_82_paths = sorted(glob.glob('./simulation_data/SEM/Case_4/82/*.png'))\n",
        "sem_4_83_paths = sorted(glob.glob('./simulation_data/SEM/Case_4/83/*.png'))\n",
        "sem_4_84_paths = sorted(glob.glob('./simulation_data/SEM/Case_4/84/*.png'))\n",
        "\n",
        "# case 1\n",
        "depth_1_80_paths = sorted(glob.glob('./simulation_data/Depth/Case_1/80/*.png') + glob.glob('./simulation_data/Depth/Case_1/80/*.png'))\n",
        "depth_1_81_paths = sorted(glob.glob('./simulation_data/Depth/Case_1/81/*.png') + glob.glob('./simulation_data/Depth/Case_1/81/*.png'))\n",
        "depth_1_82_paths = sorted(glob.glob('./simulation_data/Depth/Case_1/82/*.png') + glob.glob('./simulation_data/Depth/Case_1/82/*.png'))\n",
        "depth_1_83_paths = sorted(glob.glob('./simulation_data/Depth/Case_1/83/*.png') + glob.glob('./simulation_data/Depth/Case_1/83/*.png'))\n",
        "depth_1_84_paths = sorted(glob.glob('./simulation_data/Depth/Case_1/84/*.png') + glob.glob('./simulation_data/Depth/Case_1/84/*.png'))\n",
        "\n",
        "# case 2\n",
        "depth_2_80_paths = sorted(glob.glob('./simulation_data/Depth/Case_2/80/*.png') + glob.glob('./simulation_data/Depth/Case_2/80/*.png'))\n",
        "depth_2_81_paths = sorted(glob.glob('./simulation_data/Depth/Case_2/81/*.png') + glob.glob('./simulation_data/Depth/Case_2/81/*.png'))\n",
        "depth_2_82_paths = sorted(glob.glob('./simulation_data/Depth/Case_2/82/*.png') + glob.glob('./simulation_data/Depth/Case_2/82/*.png'))\n",
        "depth_2_83_paths = sorted(glob.glob('./simulation_data/Depth/Case_2/83/*.png') + glob.glob('./simulation_data/Depth/Case_2/83/*.png'))\n",
        "depth_2_84_paths = sorted(glob.glob('./simulation_data/Depth/Case_2/84/*.png') + glob.glob('./simulation_data/Depth/Case_2/84/*.png'))\n",
        "\n",
        "# case 3\n",
        "depth_3_80_paths = sorted(glob.glob('./simulation_data/Depth/Case_3/80/*.png') + glob.glob('./simulation_data/Depth/Case_3/80/*.png'))\n",
        "depth_3_81_paths = sorted(glob.glob('./simulation_data/Depth/Case_3/81/*.png') + glob.glob('./simulation_data/Depth/Case_3/81/*.png'))\n",
        "depth_3_82_paths = sorted(glob.glob('./simulation_data/Depth/Case_3/82/*.png') + glob.glob('./simulation_data/Depth/Case_3/82/*.png'))\n",
        "depth_3_83_paths = sorted(glob.glob('./simulation_data/Depth/Case_3/83/*.png') + glob.glob('./simulation_data/Depth/Case_3/83/*.png'))\n",
        "depth_3_84_paths = sorted(glob.glob('./simulation_data/Depth/Case_3/84/*.png') + glob.glob('./simulation_data/Depth/Case_3/84/*.png'))\n",
        "\n",
        "# case 4\n",
        "depth_4_80_paths = sorted(glob.glob('./simulation_data/Depth/Case_4/80/*.png') + glob.glob('./simulation_data/Depth/Case_4/80/*.png'))\n",
        "depth_4_81_paths = sorted(glob.glob('./simulation_data/Depth/Case_4/81/*.png') + glob.glob('./simulation_data/Depth/Case_4/81/*.png'))\n",
        "depth_4_82_paths = sorted(glob.glob('./simulation_data/Depth/Case_4/82/*.png') + glob.glob('./simulation_data/Depth/Case_4/82/*.png'))\n",
        "depth_4_83_paths = sorted(glob.glob('./simulation_data/Depth/Case_4/83/*.png') + glob.glob('./simulation_data/Depth/Case_4/83/*.png'))\n",
        "depth_4_84_paths = sorted(glob.glob('./simulation_data/Depth/Case_4/84/*.png') + glob.glob('./simulation_data/Depth/Case_4/84/*.png'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQHc-GWEI0QK"
      },
      "outputs": [],
      "source": [
        "len_80 = len(sem_1_80_paths)\n",
        "len_81 = len(sem_1_81_paths)\n",
        "len_82 = len(sem_1_82_paths)\n",
        "len_83 = len(sem_1_83_paths)\n",
        "len_84 = len(sem_1_84_paths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CUF7NHu-I3Ad"
      },
      "outputs": [],
      "source": [
        "ratio = 1\n",
        "v_ratio = 0.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S36_JResI44n"
      },
      "outputs": [],
      "source": [
        "tran_1_80 = sem_1_80_paths[int(len_80*v_ratio):int(len_80*ratio)]\n",
        "tran_1_81 = sem_1_81_paths[int(len_81*v_ratio):int(len_81*ratio)]\n",
        "tran_1_82 = sem_1_82_paths[int(len_82*v_ratio):int(len_82*ratio)]\n",
        "tran_1_83 = sem_1_83_paths[int(len_83*v_ratio):int(len_83*ratio)]\n",
        "tran_1_84 = sem_1_84_paths[int(len_84*v_ratio):int(len_84*ratio)]\n",
        "\n",
        "tran_2_80 = sem_2_80_paths[int(len_80*v_ratio):int(len_80*ratio)]\n",
        "tran_2_81 = sem_2_81_paths[int(len_81*v_ratio):int(len_81*ratio)]\n",
        "tran_2_82 = sem_2_82_paths[int(len_82*v_ratio):int(len_82*ratio)]\n",
        "tran_2_83 = sem_2_83_paths[int(len_83*v_ratio):int(len_83*ratio)]\n",
        "tran_2_84 = sem_2_84_paths[int(len_84*v_ratio):int(len_84*ratio)]\n",
        "\n",
        "tran_3_80 = sem_3_80_paths[int(len_80*v_ratio):int(len_80*ratio)]\n",
        "tran_3_81 = sem_3_81_paths[int(len_81*v_ratio):int(len_81*ratio)]\n",
        "tran_3_82 = sem_3_82_paths[int(len_82*v_ratio):int(len_82*ratio)]\n",
        "tran_3_83 = sem_3_83_paths[int(len_83*v_ratio):int(len_83*ratio)]\n",
        "tran_3_84 = sem_3_84_paths[int(len_84*v_ratio):int(len_84*ratio)]\n",
        "\n",
        "tran_4_80 = sem_4_80_paths[int(len_80*v_ratio):int(len_80*ratio)]\n",
        "tran_4_81 = sem_4_81_paths[int(len_81*v_ratio):int(len_81*ratio)]\n",
        "tran_4_82 = sem_4_82_paths[int(len_82*v_ratio):int(len_82*ratio)]\n",
        "tran_4_83 = sem_4_83_paths[int(len_83*v_ratio):int(len_83*ratio)]\n",
        "tran_4_84 = sem_4_84_paths[int(len_84*v_ratio):int(len_84*ratio)]\n",
        "\n",
        "tran_1 = tran_1_80 + tran_1_81 + tran_1_82 + tran_1_83 + tran_1_84\n",
        "tran_2 = tran_2_80 + tran_2_81 + tran_2_82 + tran_2_83 + tran_2_84\n",
        "tran_3 = tran_3_80 + tran_3_81 + tran_3_82 + tran_3_83 + tran_3_84\n",
        "tran_4 = tran_4_80 + tran_4_81 + tran_4_82 + tran_4_83 + tran_4_84\n",
        "\n",
        "tot_tran_s = tran_1 + tran_2 + tran_3 + tran_4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DFyNgHURI6zR"
      },
      "outputs": [],
      "source": [
        "dep_tran_1_80 = depth_1_80_paths[int(len_80*v_ratio):int(len_80*ratio)]\n",
        "dep_tran_1_81 = depth_1_81_paths[int(len_81*v_ratio):int(len_81*ratio)]\n",
        "dep_tran_1_82 = depth_1_82_paths[int(len_82*v_ratio):int(len_82*ratio)]\n",
        "dep_tran_1_83 = depth_1_83_paths[int(len_83*v_ratio):int(len_83*ratio)]\n",
        "dep_tran_1_84 = depth_1_84_paths[int(len_84*v_ratio):int(len_84*ratio)]\n",
        "\n",
        "dep_tran_2_80 = depth_2_80_paths[int(len_80*v_ratio):int(len_80*ratio)]\n",
        "dep_tran_2_81 = depth_2_81_paths[int(len_81*v_ratio):int(len_81*ratio)]\n",
        "dep_tran_2_82 = depth_2_82_paths[int(len_82*v_ratio):int(len_82*ratio)]\n",
        "dep_tran_2_83 = depth_2_83_paths[int(len_83*v_ratio):int(len_83*ratio)]\n",
        "dep_tran_2_84 = depth_2_84_paths[int(len_84*v_ratio):int(len_84*ratio)]\n",
        "\n",
        "dep_tran_3_80 = depth_3_80_paths[int(len_80*v_ratio):int(len_80*ratio)]\n",
        "dep_tran_3_81 = depth_3_81_paths[int(len_81*v_ratio):int(len_81*ratio)]\n",
        "dep_tran_3_82 = depth_3_82_paths[int(len_82*v_ratio):int(len_82*ratio)]\n",
        "dep_tran_3_83 = depth_3_83_paths[int(len_83*v_ratio):int(len_83*ratio)]\n",
        "dep_tran_3_84 = depth_3_84_paths[int(len_84*v_ratio):int(len_84*ratio)]\n",
        "\n",
        "dep_tran_4_80 = depth_4_80_paths[int(len_80*v_ratio):int(len_80*ratio)]\n",
        "dep_tran_4_81 = depth_4_81_paths[int(len_81*v_ratio):int(len_81*ratio)]\n",
        "dep_tran_4_82 = depth_4_82_paths[int(len_82*v_ratio):int(len_82*ratio)]\n",
        "dep_tran_4_83 = depth_4_83_paths[int(len_83*v_ratio):int(len_83*ratio)]\n",
        "dep_tran_4_84 = depth_4_84_paths[int(len_84*v_ratio):int(len_84*ratio)]\n",
        "\n",
        "dep_tran_1 = dep_tran_1_80 + dep_tran_1_81 + dep_tran_1_82 + dep_tran_1_83 + dep_tran_1_84\n",
        "dep_tran_2 = dep_tran_2_80 + dep_tran_2_81 + dep_tran_2_82 + dep_tran_2_83 + dep_tran_2_84\n",
        "dep_tran_3 = dep_tran_3_80 + dep_tran_3_81 + dep_tran_3_82 + dep_tran_3_83 + dep_tran_3_84\n",
        "dep_tran_4 = dep_tran_4_80 + dep_tran_4_81 + dep_tran_4_82 + dep_tran_4_83 + dep_tran_4_84\n",
        "\n",
        "tot_tran_d = dep_tran_1 + dep_tran_2 + dep_tran_3 + dep_tran_4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E6gfzQEeI8N4"
      },
      "outputs": [],
      "source": [
        "val_1_80 = sem_1_80_paths[:int(len_80*v_ratio)]\n",
        "val_1_81 = sem_1_81_paths[:int(len_81*v_ratio)]\n",
        "val_1_82 = sem_1_82_paths[:int(len_82*v_ratio)]\n",
        "val_1_83 = sem_1_83_paths[:int(len_83*v_ratio)]\n",
        "val_1_84 = sem_1_84_paths[:int(len_84*v_ratio)]\n",
        "\n",
        "\n",
        "val_2_80 = sem_2_80_paths[:int(len_80*v_ratio)]\n",
        "val_2_81 = sem_2_81_paths[:int(len_81*v_ratio)]\n",
        "val_2_82 = sem_2_82_paths[:int(len_82*v_ratio)]\n",
        "val_2_83 = sem_2_83_paths[:int(len_83*v_ratio)]\n",
        "val_2_84 = sem_2_84_paths[:int(len_84*v_ratio)]\n",
        "\n",
        "\n",
        "val_3_80 = sem_3_80_paths[:int(len_80*v_ratio)]\n",
        "val_3_81 = sem_3_81_paths[:int(len_81*v_ratio)]\n",
        "val_3_82 = sem_3_82_paths[:int(len_82*v_ratio)]\n",
        "val_3_83 = sem_3_83_paths[:int(len_83*v_ratio)]\n",
        "val_3_84 = sem_3_84_paths[:int(len_84*v_ratio)]\n",
        "\n",
        "\n",
        "val_4_80 = sem_4_80_paths[:int(len_80*v_ratio)]\n",
        "val_4_81 = sem_4_81_paths[:int(len_81*v_ratio)]\n",
        "val_4_82 = sem_4_82_paths[:int(len_82*v_ratio)]\n",
        "val_4_83 = sem_4_83_paths[:int(len_83*v_ratio)]\n",
        "val_4_84 = sem_4_84_paths[:int(len_84*v_ratio)]\n",
        "\n",
        "\n",
        "val_1 = val_1_80 + val_1_81 + val_1_82 + val_1_83 + val_1_84\n",
        "val_2 = val_2_80 + val_2_81 + val_2_82 + val_2_83 + val_2_84\n",
        "val_3 = val_3_80 + val_3_81 + val_3_82 + val_3_83 + val_3_84\n",
        "val_4 = val_4_80 + val_4_81 + val_4_82 + val_4_83 + val_4_84\n",
        "\n",
        "val_s = val_1 + val_2 + val_3 + val_4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4vb93ReI-zB"
      },
      "outputs": [],
      "source": [
        "dep_val_1_80 = depth_1_80_paths[:int(len_80*v_ratio)]\n",
        "dep_val_1_81 = depth_1_81_paths[:int(len_81*v_ratio)]\n",
        "dep_val_1_82 = depth_1_82_paths[:int(len_82*v_ratio)]\n",
        "dep_val_1_83 = depth_1_83_paths[:int(len_83*v_ratio)]\n",
        "dep_val_1_84 = depth_1_84_paths[:int(len_84*v_ratio)]\n",
        "dep_val_1 = dep_val_1_80 + dep_val_1_81 + dep_val_1_82 + dep_val_1_83 + dep_val_1_84\n",
        "\n",
        "dep_val_2_80 = depth_2_80_paths[:int(len_80*v_ratio)]\n",
        "dep_val_2_81 = depth_2_81_paths[:int(len_81*v_ratio)]\n",
        "dep_val_2_82 = depth_2_82_paths[:int(len_82*v_ratio)]\n",
        "dep_val_2_83 = depth_2_83_paths[:int(len_83*v_ratio)]\n",
        "dep_val_2_84 = depth_2_84_paths[:int(len_84*v_ratio)]\n",
        "dep_val_2 = dep_val_2_80 + dep_val_2_81 + dep_val_2_82 + dep_val_2_83 + dep_val_2_84\n",
        "\n",
        "dep_val_3_80 = depth_3_80_paths[:int(len_80*v_ratio)]\n",
        "dep_val_3_81 = depth_3_81_paths[:int(len_81*v_ratio)]\n",
        "dep_val_3_82 = depth_3_82_paths[:int(len_82*v_ratio)]\n",
        "dep_val_3_83 = depth_3_83_paths[:int(len_83*v_ratio)]\n",
        "dep_val_3_84 = depth_3_84_paths[:int(len_84*v_ratio)]\n",
        "dep_val_3 = dep_val_3_80 + dep_val_3_81 + dep_val_3_82 + dep_val_3_83 + dep_val_3_84\n",
        "\n",
        "dep_val_4_80 = depth_4_80_paths[:int(len_80*v_ratio)]\n",
        "dep_val_4_81 = depth_4_81_paths[:int(len_81*v_ratio)]\n",
        "dep_val_4_82 = depth_4_82_paths[:int(len_82*v_ratio)]\n",
        "dep_val_4_83 = depth_4_83_paths[:int(len_83*v_ratio)]\n",
        "dep_val_4_84 = depth_4_84_paths[:int(len_84*v_ratio)]\n",
        "dep_val_4 = dep_val_4_80 + dep_val_4_81 + dep_val_4_82 + dep_val_4_83 + dep_val_4_84\n",
        "\n",
        "val_dep = dep_val_1 + dep_val_2 + dep_val_3 + dep_val_4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "agaybxiFJAcp"
      },
      "outputs": [],
      "source": [
        "train_sem_paths = tot_tran_s\n",
        "train_depth_paths = tot_tran_d\n",
        "\n",
        "val_sem_paths = val_s\n",
        "val_depth_paths = val_dep"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformedDataset(Dataset):           \n",
        "    def __init__(self, sem_path_list, depth_path_list, transform = None):\n",
        "        self.sem_path_list = sem_path_list\n",
        "        self.depth_path_list = depth_path_list\n",
        "        self.transform = transform\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        sem_path = self.sem_path_list[index]\n",
        "        sem_img = PIL.Image.open(sem_path).convert(\"L\")\n",
        "        if(self.transform):                                                 \n",
        "            sem_img = self.transform(sem_img)\n",
        "        sem_img = np.array(sem_img)\n",
        "        sem_img = np.expand_dims(sem_img, axis=-1).transpose(2,0,1)\n",
        "        sem_img = sem_img / 255.\n",
        "\n",
        "        if self.depth_path_list is not None:\n",
        "            depth_path = self.depth_path_list[index]\n",
        "            depth_img = PIL.Image.open(depth_path).convert(\"L\")\n",
        "            if(self.transform):                                       \n",
        "                depth_img = self.transform(depth_img)\n",
        "            depth_img = np.array(depth_img)\n",
        "            depth_img = np.expand_dims(depth_img, axis=-1).transpose(2,0,1)\n",
        "            depth_img = depth_img / 255.\n",
        "\n",
        "            return torch.Tensor(sem_img), torch.Tensor(depth_img)  \n",
        "        \n",
        "        else:\n",
        "            img_name = sem_path.split('/')[-1]                                  \n",
        "            \n",
        "            return torch.Tensor(sem_img), img_name                     \n",
        "          \n",
        "            \n",
        "    def __len__(self):\n",
        "        return len(self.sem_path_list)"
      ],
      "metadata": {
        "id": "wj2dZffZ9wtp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "jitterTransform08 = transforms.ColorJitter(brightness = [0.8,0.8])\n",
        "jitterTransform09 = transforms.ColorJitter(brightness = [0.9,0.9])\n",
        "\n",
        "t_jitter08 = TransformedDataset(train_sem_paths,train_depth_paths, transform = jitterTransform08)\n",
        "t_jitter09 = TransformedDataset(train_sem_paths,train_depth_paths, transform = jitterTransform09)\n"
      ],
      "metadata": {
        "id": "OaSDFJ3S9ZdG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oL5ciQcpB91m"
      },
      "outputs": [],
      "source": [
        "t_origindataset = TransformedDataset(train_sem_paths, train_depth_paths,None)\n",
        "\n",
        "train_dataset = t_origindataset + t_jitter08 + t_jitter09 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8cABjo__cFeA"
      },
      "outputs": [],
      "source": [
        "v_origindataset = TransformedDataset(val_sem_paths, val_depth_paths,None)\n",
        "\n",
        "valid_dataset = v_origindataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5he0-Rqb6dX"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size = CFG['BATCH_SIZE'], shuffle = True, num_workers = 6)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size = CFG['BATCH_SIZE'], shuffle = False, num_workers = 6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B3jXVZIAiFyr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd21ee77-2fca-42fb-b7ee-c03db9b4001a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(415956, 34652)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "len(train_dataset), len(valid_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train-SEM 데이터\n",
        "sem_paths = sorted(glob.glob('/content/train/SEM/*/*/*.png'), key=lambda x : x[-14:-4])  \\\n",
        "\n",
        "depth_paths = sorted(glob.glob(\"/content/train_to_depth/*.png\"))\n",
        "\n",
        "sem_paths[:5], depth_paths[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4T28zD6FCaS",
        "outputId": "ae19ab6b-0e2b-4f1f-f360-27bbed835d22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['/content/train/SEM/Depth_140/site_00201/SEM_000001.png',\n",
              "  '/content/train/SEM/Depth_140/site_00413/SEM_000002.png',\n",
              "  '/content/train/SEM/Depth_140/site_00097/SEM_000005.png',\n",
              "  '/content/train/SEM/Depth_140/site_00099/SEM_000006.png',\n",
              "  '/content/train/SEM/Depth_140/site_00475/SEM_000008.png'],\n",
              " [])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train : val = 8 : 2 구분\n",
        "add_train_sem_paths = sem_paths[:int(len(sem_paths)*0.8)]\n",
        "add_val_sem_paths = sem_paths[int(len(sem_paths)*0.8):]\n",
        "\n",
        "add_train_depth_paths = depth_paths[:int(len(sem_paths)*0.8)]\n",
        "add_val_depth_paths = depth_paths[int(len(sem_paths)*0.8):]"
      ],
      "metadata": {
        "id": "5wMyM9-aFEwj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "at_origindataset = TransformedDataset(add_train_sem_paths, add_train_depth_paths) \n",
        "\n",
        "add_train_dataset = at_origindataset\n",
        "# add_train_dataset 생성"
      ],
      "metadata": {
        "id": "0aZKLLZWE4K4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "av_origindataset = TransformedDataset(add_val_sem_paths,add_val_depth_paths) \n",
        "\n",
        "add_valid_dataset = av_origindataset\n",
        "# add_valid_dataset 생성"
      ],
      "metadata": {
        "id": "2-By1hZjFEQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "add_train_loader = DataLoader(add_train_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=True, num_workers=6)\n",
        "\n",
        "add_val_loader = DataLoader(add_valid_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False, num_workers=6)"
      ],
      "metadata": {
        "id": "V3hy3ywpFHJC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39962463-032f-490a-a76d-c03991795f38"
      },
      "source": [
        "## Model Define\n",
        "\n",
        "                   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qDAh-1boPDfo"
      },
      "outputs": [],
      "source": [
        "class NewModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NewModel, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(1,64,3, stride=2, padding = 1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64,128,3, stride=2, padding = 1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128,256,3,stride=2, padding = 1),\n",
        "            nn.ReLU()\n",
        "            )\n",
        "        \n",
        "        self.flatten = nn.Sequential(nn.Flatten(start_dim = 1), nn.Dropout(0.2))\n",
        "\n",
        "        self.unflatten = nn.Unflatten(dim=1, unflattened_size = (256,9,6))\n",
        "        \n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(256,128,3,stride=2,padding = 1,output_padding = 1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(128,64,3, stride=2, padding = 1, output_padding = 1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64,1,3, stride=2, padding = 1, output_padding = 1)\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "\n",
        "        x = self.flatten(x)\n",
        "\n",
        "        x = self.unflatten(x)\n",
        "\n",
        "        x = self.decoder(x)                                                    \n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "122af0aa-a1fd-4595-9488-35761e3cb596"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a17df6b3-16c9-44dd-b0fd-ffb501fee749"
      },
      "outputs": [],
      "source": [
        "def train(model, optimizer, train_loader, val_loader, scheduler, epochs, device):  \n",
        "    model.to(device)                                              \n",
        "    criterion_img = nn.L1Loss().to(device)   \n",
        "    best_score = 999999\n",
        "    best_model = None\n",
        "    best_hist = []\n",
        "    for epoch in range(1, epochs + 1):                \n",
        "        model.train()\n",
        "       \n",
        "        for sem, depth in tqdm(iter(train_loader)):                           \n",
        "            sem = sem.float().to(device)                               \n",
        "            depth = depth.float().to(device)\n",
        "            \n",
        "            optimizer.zero_grad()                                 \n",
        "            model_pred_img = model(sem)                                    \n",
        "            loss_img = criterion_img(model_pred_img, depth)               \n",
        "            loss_img.backward()                                        \n",
        "            optimizer.step()                                                    \n",
        "            train_loss.append(loss_img.item())                            \n",
        "            \n",
        "        val_img_loss, val_img_rmse = validation(model, criterion_img,val_loader, device) \n",
        "        \n",
        "        if best_score > val_img_rmse:                                \n",
        "            best_score = val_img_rmse\n",
        "            best_model = model\n",
        "            best_hist = [epoch, np.mean(train_loss), val_img_rmse]\n",
        "            best_rmse.append(best_score)\n",
        "            \n",
        "        if scheduler is not None:                                               \n",
        "            scheduler.step()\n",
        "    \n",
        "    return best_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c062b034-5b9b-4d80-b8e3-e781fbd4845b"
      },
      "outputs": [],
      "source": [
        "def validation(model, criterion_img, val_loader, device):                 \n",
        "    model.eval()                                                     \n",
        "    b_rmse_img = nn.MSELoss().to(device)                                      \n",
        "    \n",
        "\n",
        "    with torch.no_grad():                                                     \n",
        "        for sem, depth in tqdm(iter(val_loader)):                             \n",
        "            sem = sem.float().to(device)                        \n",
        "            depth = depth.float().to(device)\n",
        "            \n",
        "            model_pred_img = model(sem)                                     \n",
        "            loss_img = criterion_img(model_pred_img, depth)                  \n",
        "            pred = (model_pred_img*255.).type(torch.int8).float()           \n",
        "            true = (depth*255.).type(torch.int8).float()                      \n",
        "\n",
        "            b_rmse_img = torch.sqrt(criterion_img(pred, true))      \n",
        "\n",
        "            val_img_loss.append(loss_img.item())                            \n",
        "            val_img_rmse.append(b_rmse_img.item())\n",
        "\n",
        "            # Total\n",
        "            total_loss = loss_img\n",
        "            b_rmse_total = torch.sqrt((total_loss))\n",
        "\n",
        "    return np.mean(val_img_loss), np.mean(val_img_rmse)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51da39f9-904f-4abd-a7d2-cdf29c4a6c24"
      },
      "source": [
        "## Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86142d9a-68b7-4d04-8423-49d28025411d"
      },
      "outputs": [],
      "source": [
        "model = NewModel()\n",
        "model.eval()\n",
        "optimizer = torch.optim.Adam(params = model.parameters(), lr = CFG[\"LEARNING_RATE\"])\n",
        "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda = lambda epoch: epoch)                                  \n",
        "\n",
        "infer_model = train(model, optimizer, train_loader, valid_loader, scheduler, CFG['EPOCHS'], device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "axmLNjNfXSDn"
      },
      "outputs": [],
      "source": [
        "    df_loss=pd.Series(train_loss)\n",
        "    print(df_loss)\n",
        "    df_loss.plot()\n",
        "\n",
        "    best=pd.Series(best_rmse)\n",
        "    print(best)\n",
        "    best.plot()\n",
        "\n",
        "    val_loss=pd.Series(val_img_loss)\n",
        "    print(val_loss[int(2165):])\n",
        "    val_loss[int(2165):].plot()\n",
        "\n",
        "    val_rmse=pd.Series(val_img_rmse)\n",
        "    print(val_rmse)\n",
        "    val_rmse[int(2165):].plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Verify"
      ],
      "metadata": {
        "id": "a-6VseQyNQUR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_1(model, optimizer, train_loader, val_loader, scheduler, device):    \n",
        "    model.to(device)                                              \n",
        "    criterion_img = nn.L1Loss().to(device)   \n",
        "                                                                                         \n",
        "    best_score = 999999\n",
        "    best_model = None\n",
        "    best_hist = []\n",
        "    for epoch in range(1, CFG['EPOCHS']+1):                         \n",
        "        model.eval()\n",
        "        train_loss = []                                                      \n",
        "       \n",
        "        for sem, depth in tqdm(iter(train_loader)):                       \n",
        "                                                                         \n",
        "            sem = sem.float().to(device)                                \n",
        "            depth = depth.float().to(device)\n",
        "            optimizer.zero_grad()                                         \n",
        "            \n",
        "            model_pred_img = model(sem)                                    \n",
        "            \n",
        "            loss_img = criterion_img(model_pred_img, depth)                \n",
        "            \n",
        "            total_loss = loss_img\n",
        "\n",
        "            total_loss.backward()\n",
        "\n",
        "            optimizer.step()                                                    \n",
        "            \n",
        "            train_loss.append(total_loss.item())                            \n",
        "            \n",
        "        \n",
        "        val_img_loss, val_img_rmse = validation(model, criterion_img,val_loader,device) \n",
        "    \n",
        "        if best_score > val_img_rmse:                                     \n",
        "            best_score = val_img_rmse\n",
        "            best_model = model\n",
        "            best_hist = [epoch, np.mean(train_loss), val_img_rmse]\n",
        "            best_rmse.append(best_score)\n",
        "        \n",
        "        if scheduler is not None:                                               \n",
        "            scheduler.step()\n",
        "    \n",
        "    return best_model"
      ],
      "metadata": {
        "id": "0U_UF6E3A5l-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validation_1(model, criterion_img, val_loader,device):                  \n",
        "    model.eval()                                                             \n",
        "    b_rmse_img = nn.MSELoss().to(device)                               \n",
        "    \n",
        "    val_img_loss = []                                                      \n",
        "    val_img_rmse = []\n",
        "\n",
        "\n",
        "    with torch.no_grad():                                               \n",
        "        for sem, depth in tqdm(iter(val_loader)):                         \n",
        "            sem = sem.float().to(device)                                  \n",
        "            depth = depth.float().to(device)\n",
        "            \n",
        "            model_pred_img = model(sem)                                     \n",
        "            loss_img = criterion_img(model_pred_img, depth)                \n",
        "            # image \n",
        "            pred = (model_pred_img*255.).type(torch.int8).float()        \n",
        "            true = (depth*255.).type(torch.int8).float()                    \n",
        "\n",
        "            b_rmse_img = torch.sqrt(criterion_img(pred, true))                \n",
        "\n",
        "            val_img_loss.append(loss_img.item())                            \n",
        "            val_img_rmse.append(b_rmse_img.item())\n",
        "\n",
        "            # Total\n",
        "    return np.mean(val_img_loss), np.mean(val_img_rmse)"
      ],
      "metadata": {
        "id": "HBclCMakA5fz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Model = infer_model\n",
        "Model.eval()\n",
        "optimizer = torch.optim.Adam(params = Model.parameters(), lr = 1e-3)            \n",
        "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda = lambda epoch: epoch)\n",
        "\n",
        "infer_model = train_1(Model, optimizer, add_train_loader, add_val_loader, scheduler,device)"
      ],
      "metadata": {
        "id": "J7I6yswGAeiM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    df_loss=pd.Series(train_loss)\n",
        "    print(df_loss)\n",
        "    df_loss.plot()\n",
        "\n",
        "    best=pd.Series(best_rmse)\n",
        "    print(best)\n",
        "    best.plot()\n",
        "\n",
        "    val_loss=pd.Series(val_img_loss)\n",
        "    print(val_loss[int(2165):])\n",
        "    val_loss[int(2165):].plot()\n",
        "\n",
        "    val_rmse=pd.Series(val_img_rmse)\n",
        "    print(val_rmse)\n",
        "    val_rmse[int(2165):].plot()"
      ],
      "metadata": {
        "id": "KJsFmHz3A3Sq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}